{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the current GPU usage. Please try to be nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct  6 10:20:16 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.56       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  On   | 00000000:18:00.0 Off |                  N/A |\n",
      "| 50%   83C    P2   251W / 250W |   2304MiB / 11019MiB |     82%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  On   | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 29%   33C    P8    20W / 250W |      3MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 3090    On   | 00000000:AF:00.0 Off |                  N/A |\n",
      "|  0%   38C    P8    30W / 350W |   3362MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    322957      C   python                           2301MiB |\n",
      "|    2   N/A  N/A    264280      C   ...s/june2020-gpu/bin/python     3359MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **WARNING**: The card numbers here are *not* the same as in CUDA. You have been warned. However, these numbers are correct if you use the `select_gpu` helper function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import inspect\n",
    "import pandas as pd\n",
    "\n",
    "# Python 3 standard library\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from model.collectdata import collect_data\n",
    "from model.loss import Loss\n",
    "from model.training import trainNet, select_gpu, Results\n",
    "from model.plots import dual_train_plots, replace_in_ax\n",
    "from model.core import PVModel, write_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined:\n",
    "from model.models import SimpleCNN3Layer as Model\n",
    "\n",
    "#class Model(PVModel):\n",
    "#    INPUTS = 3\n",
    "#    KERNEL_SIZE =   [25, 15, 5]\n",
    "#    CHANNELS_SIZE = [5, 10, 1]\n",
    "#    DEFAULTS = {'dropout_1':0.15, 'dropout_2':0.15, 'dropout_3':0.25}\n",
    "#    FC = True\n",
    "#    FINAL_ACTIVATION = nn.Softplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up local parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 80\n",
    "\n",
    "# Name is the output file name\n",
    "name = 'Feb05_mask_16K_z_3layer'\n",
    "\n",
    "# Make an output folder named \"name\" (change if you want)\n",
    "output = Path(name)\n",
    "\n",
    "# Size of batches\n",
    "batch_size = 128\n",
    "\n",
    "# How fast to learn\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the output directory if it does not exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "output.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare output dataframe to be filled in during run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gets built up during the run - do not rerun this cell\n",
    "results = pd.DataFrame([], columns=Results._fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model source code information into the output directory. Only PVModel subclasses are supported for in-cell definitions; other models need to be in a `.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_model(output / f'{name}_model_info.py', Model, Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU selection\n",
    "\n",
    "Set up Torch device configuration. All tensors and model parameters need to know where to be put.\n",
    "This takes a BUS ID number: The BUS ID is the same as the listing at the top of this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = select_gpu(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Load the dataset, split into parts, then move to device if `device=device` is present. If this line is commented out, then load the datasets as the calculations progress. Allows larger datasets and plays nicer with memory, but very slightly slower. See `collectdata.py` in the `../model` directory for the source. Datasets are listed in the model directory README, repeated here:\n",
    "\n",
    "|        From       |          To         |         Events          |\n",
    "|-------------------|---------------------|-------------------------|\n",
    "| `kernel_20181003` | `Oct03_20K_val`     | 1,2                     |\n",
    "| `kernel_20181003` | `Oct03_20K_test`    | 3,4                     |\n",
    "| `kernel_20181003` | `Oct03_40K_train`   | 5,6,7,8                 |\n",
    "| `kernel_20181003` | `Oct03_80K_train`   | 9,10,11,12,13,14,15,16  |\n",
    "| `kernel_20181003` | `Oct03_80K2_train`  | 17,18,19,20,21,22,23,24 |\n",
    "| `kernel_20180814` | `Aug14_80K_train`   | 1,2,3,4,5,6,7,8         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset. You can put as many files here as desired.\n",
    "train_loader = collect_data('data/Oct03_80K_train.h5',\n",
    "                            'data/Oct03_80K2_train.h5',\n",
    "                            batch_size=batch_size,\n",
    "                            #device=device,\n",
    "                            #load_xy=True,\n",
    "                            masking=True, shuffle=True)\n",
    "\n",
    "# Validation dataset. You can slice to reduce the size.\n",
    "val_loader = collect_data('data/Oct03_20K_val.h5',\n",
    "                          batch_size=batch_size,\n",
    "                          slice=slice(256 * 39),\n",
    "                          #load_xy=True,\n",
    "                          #device=device,\n",
    "                          masking=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a model, use multiple GPUs if they are VISIBLE, and move the model to the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = Model() # optional: dropout_1 = 0.15, etc.\n",
    "loss = Loss(epsilon=1e-5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should support multi-gpu, but doesn't work very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move the model's weight matricies to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "The body of this loop runs once per epoch. Results is a named tuple of values (loss per epoch for training and validation, time each). Start by setting up a plot first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, tax, lax, lines = dual_train_plots()\n",
    "fig = ax.figure\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for result in trainNet(model, optimizer, loss,\n",
    "                        train_loader, val_loader,\n",
    "                        n_epochs, epoch_start=len(results),\n",
    "                        notebook=True):\n",
    "    \n",
    "    results = results.append(pd.Series(result._asdict()), ignore_index=True)\n",
    "    \n",
    "    xs = results.index\n",
    "    \n",
    "    # Update the plot above\n",
    "    lines['train'].set_data(results.index,results.cost)\n",
    "    lines['val'].set_data(results.index,results.val)\n",
    "    \n",
    "    #filter first cost epoch (can be really large)\n",
    "    max_cost = max(max(results.cost if len(results.cost)<2 else results.cost[1:]), max(results.val))\n",
    "    min_cost = min(min(results.cost), min(results.val))\n",
    "    \n",
    "    # The plot limits need updating too\n",
    "    ax.set_ylim(min_cost*.9, max_cost*1.1)  \n",
    "    ax.set_xlim(-.5, len(results.cost) - .5)\n",
    "    \n",
    "    replace_in_ax(lax, lines['eff'], xs, results['eff_val'].apply(lambda x: x.eff_rate))\n",
    "    replace_in_ax(tax, lines['fp'], xs, results['eff_val'].apply(lambda x: x.fp_rate))\n",
    "    \n",
    "    # Redraw the figure\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # Save each model state dictionary\n",
    "    torch.save(model.state_dict(), output / f'{name}_{result.epoch}.pyt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Let's save some results: (even though if you have not changed the code above, it saves the model every epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and save the final model (even though it was also saved above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), output / f'{name}_final.pyt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the output results (ignore the warning about pickeling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results.to_hdf(f'{name}_stats.hdf5', 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the plot (remake the plot just in case the one above has broken):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dual_train_plots(results.index,\n",
    "                 results.cost, results.val, \n",
    "                 results['eff_val'].apply(lambda x: x.eff_rate),\n",
    "                 results['eff_val'].apply(lambda x: x.fp_rate))\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(output / f'{name}_stats_a.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quit the kernel (try to be nice to other users). Note that plots will vanish (but are saved, so that's okay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
